# T5
## T5 = Text to Text Transfer from Transformers, which was proven to perform extremely well in multi NLP tasks.
Goal:
- Implement the code neccesary for Bidirectional Encoder Representation from Transformer (BERT). 
- Understand how the C4 dataset is structured. 
- Use a pretrained model for inference. 
- Understand how the "Text to Text Transfer from Transformers" or T5 model works.
